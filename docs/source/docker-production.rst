CGAP-Docker (Production)
========================

CGAP-Docker runs in production on AWS Elastic Container Service, meant to be orchestrated from the 4dn-cloud-infra repository. End users will modify ``deploy/docker/production/Makefile`` to suite their immediate build needs with respect to target AWS Account/ECR Repository/Tagging strategy. For more information on the specifics of the ECS setup, see 4dn-cloud-infra.

The CGAP Application has been orchestrated into the ECS Service/Task paradigm. As of writing all core application services have their own image tag varied by passing the ``$ENTRYPOINT`` build argument. As such, they are all separate services with the following notable characteristics:

    * WSGI - services standard API requests - 8x parallelization on Fargate Spot
    * Indexer - hits /index at 3 second intervals indefinitely - 4x parallelization on Fargate Spot
    * Ingester - poll for ingestion tasks from SQS - 1x parallelization TODO add ability to add additional tasks through API
    * Deployment - triggers the standard deployment actions - must be explicitly run either through ECS console or TODO through API.

Building an Image
^^^^^^^^^^^^^^^^^

The production application configuration is in ``deploy/docker/production``. A description of all the relevant files follows.

    * Dockerfile - at repo top level - configurable file that builds both all local and production images.
    * docker-compose.yml - at repo top level - configures the local deployment - unused in production.
    * assume_identity.py - script for pulling application configuration from Secrets Manager. Note that this secret is meant to be generated by the Datastore stack in 4dn-cloud-infra and manually filled out.
    * entrypoint.sh - WSGI entrypoint
    * entrypoint_deployment.sh - deployment entrypoint
    * entrypoint_indexer.sh - indexer entrypoint
    * entrypoint_ingester.sh - ingester entrypoint
    * install_nginx.sh - script for pulling in nginx
    * Makefile - configures builds/pushes for relevant images/tags
    * mastertest.ini - base ini file used to build production.ini on the server
    * nginx.conf - nginx configuration


The following instructions describe how to build and push images. Note though that we assume an existing ECS setup. For instructions on how to orchestrate ECS, see 4dn-cloud-infra, but that is not the focus of this documentation.

    1. Ensure the orchestrator credentials are sourced, or that your IAM user has been granted sufficient perms to push to ECR.
    2. In the Makefile, replace "cgap-mastertest" with the env.name configured for the environment. This name should match the ECR repo name if you navigate to the ECR Console.
    3. Again in the Makefile, replace the ECR Repo URL (NOT the tags) with the one from the output of the ECR stack in the account.
    4. Run ``make login``, which should pull ECR credentials using the currently active AWS credentials.
    5. Run ``make info`` for information on tags.
    6. Run the appropriate make target to build/deploy the desired version by pushing a new image to ECR. Note that the upload process may take a long time if you made application code (non-configuration) changes.


Note that steps 1, 4 and 6 are all that are needed to be repeated once initial setup is done, assuming you are continuously pushing to the same location. To change which ECS orchestration you are effectively deploying to, all steps must be repeated in the relevant account.


Tagging Strategy
^^^^^^^^^^^^^^^^

As stated previously, the cgap-portal consists of 4 core components that have been translated into ECS Services/Tasks. The task definitions specify image tags that differentiate each of the 4 components. Those tags are:

    * ``latest`` - push to this tag to modify the WSGI image
    * ``latest-indexer`` - push to this tag to modify the indexer image
    * ``latest-ingester`` - push to this tag to modify the ingester image
    * ``latest-deployment`` - push to this tag to modify the deployment image

After all new image versions have been pushed, issue a forced deployment update to the ECS cluster. This action will spawn a new set of tasks for all services using the newer image tags. For WSGI, once the new tasks are deemed healthy by ECS and the Application Load Balancer, they will be added to the WSGI Target Group and immediately begin serving requests. At that time the old tasks will begin the de-registration process from the target group, after which they will be spun down. The remaining new tasks will come online more quickly since they do not need to pass load balancer health checks. Once the old tasks have been cleaned up, it is safe to trigger a deployment task through the Deployment Service.

Deciding what Image(s) to Update
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Generally speaking, all 4 tags must be pushed in order for the deployment to be considered "valid". With that said, there are a few scenarios where it may make sense to do a partial image bundle upload. Note that when uploading to production, none of the following apply.

    * If you are only making front-end changes, feel free to push only the ``latest`` tag.
    * If you are not ingesting VCF files or modifying the ingestion pipeline, you do not need to update the ``latest-ingester`` image.
    * If you want to phase back-end changes in prior to releasing them on the front-end, push ``latest-indexer`` and ``latest-deployment`` images.
    * If you are only modifying the deployment commands ie: ``entrypoint_deployment.sh``, then you only need to update the ``latest-deployment`` tag.


Note that these images have implied dependencies on each other that are not obvious. We consider the deployment and indexer images to be bundled together in the sense that updating one without the other will trigger undefined behavior if the deployment is triggered. It is thus imperative that, at a bare minimum before triggering an ECS Cluster update, that both the ``latest-indexer`` and ``latest-deployment`` tags have been updated in the case that any back-end changes have been made, whether it be to application, it's dependencies or the Dockerfile itself.
